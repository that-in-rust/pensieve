Convert a large text file to smaller chunks of equal size

"Hey, ever split a pizza into equal slices? That's exactly what we're doing with text files!"

🎯 Goal: Split a large text file into smaller chunks of equal size (N MB each)

📋 PRD (Product Requirements Document):
1. Input: Accept any text file regardless of size
2. Output: Generate files of equal size of N MB each as shared by user in cargo run N (±1KB tolerance)
3. Performance: Process 1GB file in under 30 seconds
4. Memory: Max usage = 2x buffer size (default 1MB)
5. Recovery: Auto-resume on failure, cleanup incomplete files
6. CLI Interface: Simple flags (-i input -s size -o output_dir)
7. Progress: Show real-time progress bar and ETA
8. Validation: Verify output files match input checksum

System Architecture:
```
Input File (big.txt)           Output Files
┌──────────────┐              ┌────────────┐
│              │         ┌───▶│ chunk_1.txt│
│   Large      │         │    └────────────┘
│   Text       │─────────┤    ┌────────────┐
│   File       │         ├───▶│ chunk_2.txt│
│              │         │    └────────────┘
└──────────────┘         │    ┌────────────┐
                         └───▶│ chunk_n.txt│
                              └────────────┘
```

Code Structure:
```rust
src/
├── main.rs         // CLI handling & entry point
├── chunker.rs      // File splitting logic
├── progress.rs     // Progress bar & ETA tracking
└── utils.rs        // Helper functions & checksums

// Flow of execution:
┌─────────────┐     ┌──────────────┐     ┌────────────┐
│ Parse CLI   │────▶│ Read Source  │────▶│ Calculate  │
│ -i -s -o    │     │ File Stats   │     │ Chunks     │
└─────────────┘     └──────────────┘     └─────┬──────┘
                                               │
┌─────────────┐     ┌──────────────┐          │
│ Write &     │◀────│ Split Into   │◀─────────┘
│ Track Progress    │ Chunks       │
└─────────────┘     └──────────────┘
```

Example Usage:
```rust
fn main() {
    let args = Args::parse();  // Uses clap for CLI parsing
    println!("🔨 File Splitter Starting...");
    
    let pb = ProgressBar::new(total_bytes);
    let chunks = split_file(&args.input, args.size_mb, &args.output_dir, &pb)?;
    
    println!("✅ Created {} chunks", chunks.len());
    println!("📂 Output directory: {}", args.output_dir);
}
```

Memory Management:
```
Buffer Strategy:
┌────────────────┐
│ Read Buffer    │ 1MB
├────────────────┤
│ Write Buffer   │ 1MB
└────────────────┘

                    ┌─── Controlled memory usage
                    │
┌──────────┐    ┌──┴───┐    ┌──────────┐
│ Input    │───▶│Buffer│───▶│ Output   │
│ Stream   │    │      │    │ Stream   │
└──────────┘    └──────┘    └──────────┘
```

Error Handling:
```
┌─────────────┐
│ User Error  │──┐
└─────────────┘  │
┌─────────────┐  │    ┌───────────────┐
│ File Error  │──┴───▶│ Error Handler │
└─────────────┘       └───────────────┘
┌─────────────┐  ┌────┘
│ IO Error    │──┘
└─────────────┘
```

Performance Considerations:
```
🚀 Speed Optimizations:
┌────────────────────┐
│ Buffered Reading   │ Reduces system calls
├────────────────────┤
│ Parallel Writing   │ Multiple chunks at once
├────────────────────┤
│ Memory Mapping     │ For very large files
└────────────────────┘
```

💡 Key Features:
- Handles files larger than available RAM
- Progress reporting
- Checksum verification
- Resume capability
- Clean error handling

🔍 Design Choices:
- Uses BufReader/BufWriter for efficiency
- Streams data instead of loading entire file
- Maintains original file permissions
